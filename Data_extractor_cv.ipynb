{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T07:33:37.195721Z",
     "start_time": "2025-02-25T07:33:30.111643Z"
    }
   },
   "cell_type": "code",
   "source": "pip install openpyxl",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\minh.khuatduynhat\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T07:33:54.047289Z",
     "start_time": "2025-02-25T07:33:54.043773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_folder = 'EN'  # Update with your folder path\n",
    "output_folder = 'output'  # Folder to save extracted sections\n",
    "# List of section names to look for (in order)\n",
    "section_names = [\n",
    "    'PERSONAL DETAIL',\n",
    "    'ABOUT',\n",
    "    'EDUCATIONAL BACKGROUND',\n",
    "    'CERTIFICATION',\n",
    "    'LANGUAGE',\n",
    "    'TECHNICAL SKILL',\n",
    "    'EXPERIENCE'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T07:33:55.820167Z",
     "start_time": "2025-02-25T07:33:55.815647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_accent(text):\n",
    "    \"\"\"Removes Vietnamese accents from a string.\"\"\"\n",
    "    text = re.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', text)\n",
    "    text = re.sub(r'[ÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴ]', 'A', text)\n",
    "    text = re.sub(r'[èéẹẻẽêềếệểễ]', 'e', text)\n",
    "    text = re.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', text)\n",
    "    text = re.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', text)\n",
    "    text = re.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', text)\n",
    "    text = re.sub(r'[ìíịỉĩ]', 'i', text)\n",
    "    text = re.sub(r'[ÌÍỊỈĨ]', 'I', text)\n",
    "    text = re.sub(r'[ùúụủũưừứựửữ]', 'u', text)\n",
    "    text = re.sub(r'[ÙÚỤỦŨƯỪỨỰỬỮ]', 'U', text)\n",
    "    text = re.sub(r'[ỳýỵỷỹ]', 'y', text)\n",
    "    text = re.sub(r'[ỲÝỴỶỸ]', 'Y', text)\n",
    "    text = re.sub(r'[đ]', 'd', text)\n",
    "    text = re.sub(r'[Đ]', 'D', text)\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to find the start row of a section\n",
    "def find_section_row(section_name):\n",
    "  try:\n",
    "      return data[data.apply(lambda x: x.str.contains(section_name, case=True, na=False)).any(axis=1)].index[0]\n",
    "  except IndexError:\n",
    "      return None  # Return None if the section is not found\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "  if filename.endswith('.xlsx'):  # Only process Excel files\n",
    "    input_file = os.path.join(input_folder, filename)\n",
    "    output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_sections.txt\")\n",
    "  # Read the Excel file\n",
    "    data = pd.read_excel(input_file, header=None)  # Read without a header since this is structured data\n",
    "    data = data.astype(str)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    data = data.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Find start rows for all sections\n",
    "    sections = {name: find_section_row(name) for name in section_names}\n",
    "\n",
    "    # Filter out missing sections and sort by row index\n",
    "    sections = {name: row for name, row in sections.items() if row is not None}\n",
    "    sorted_sections = sorted(sections.items(), key=lambda x: x[1])  # Sort by start row\n",
    "\n",
    "    # Add a pseudo end row for the last section\n",
    "    end_row = find_section_row('I declare that the above information is accurate.')\n",
    "    if end_row is None:\n",
    "        end_row = data.shape[0]  # Default to the last row of the data if the end row is not found\n",
    "\n",
    "    # Process each section\n",
    "    output_content = []\n",
    "    for i, (section_name, start_row) in enumerate(sorted_sections):\n",
    "        # Determine the end row for this section\n",
    "        next_start_row = sorted_sections[i + 1][1] if i + 1 < len(sorted_sections) else end_row\n",
    "        section_data = data.iloc[start_row:next_start_row, :]\n",
    "\n",
    "        # Combine rows into a single string\n",
    "        section_text = section_data.apply(lambda row: ' '.join(row.dropna()), axis=1)\n",
    "        section_content = '\\n'.join([line.replace('nan', '').replace('- ', '').replace(' – ', '~').strip() for line in section_text if line.strip()]).strip()\n",
    "\n",
    "        # Remove Vietnamese accents\n",
    "        section_content = remove_accent(section_content)\n",
    "\n",
    "        # Append the section content to the output\n",
    "        output_content.append(f\"{section_content}\\n\")\n",
    "\n",
    "    # Save the output to a text file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(output_content)\n",
    "\n",
    "    print(f\"Extracted content has been saved to {output_file}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0u_zS-p-8g5",
    "outputId": "c5662847-5ced-4034-aa09-488bf1e18b6c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737367473094,
     "user_tz": -420,
     "elapsed": 14114,
     "user": {
      "displayName": "snowsnow000",
      "userId": "06354588174686678047"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-02-25T07:34:04.882580Z",
     "start_time": "2025-02-25T07:33:57.286372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted content has been saved to output\\Copy of CV-Vu-Viet-Anh_sections.txt\n",
      "Extracted content has been saved to output\\Copy of VTI - CV Bui Duc Anh_sections.txt\n",
      "Extracted content has been saved to output\\CV - TO QUY THANH_sections.txt\n",
      "Extracted content has been saved to output\\CV Pham Viet Hieu - Cloud SA1_sections.txt\n",
      "Extracted content has been saved to output\\CV-Doan-Dinh-Vu-Cong_sections.txt\n",
      "Extracted content has been saved to output\\CV-Le Huu Minh_sections.txt\n",
      "Extracted content has been saved to output\\CV-Le Sy Quang_sections.txt\n",
      "Extracted content has been saved to output\\CV-Ngo-Thach-Anh_sections.txt\n",
      "Extracted content has been saved to output\\CV-Nguyen Phi Hai Nam_sections.txt\n",
      "Extracted content has been saved to output\\CV-Nguyen Thi Hong Nhung_sections.txt\n",
      "Extracted content has been saved to output\\CV-NguyenHaiMy-EN_sections.txt\n",
      "Extracted content has been saved to output\\CV-NguyenQuangHoa_sections.txt\n",
      "Extracted content has been saved to output\\CV-Pham-Thi-Minh-Luong-PM _sections.txt\n",
      "Extracted content has been saved to output\\CV-Pham-Thi-Minh_Luong_Automation Test_2024_sections.txt\n",
      "Extracted content has been saved to output\\CV-Vu-Viet-Anh_sections.txt\n",
      "Extracted content has been saved to output\\CV_Do Van Huan_sections.txt\n",
      "Extracted content has been saved to output\\CV_Luong Van Huan_sections.txt\n",
      "Extracted content has been saved to output\\CV_Nguyen Thanh Phuong_sections.txt\n",
      "Extracted content has been saved to output\\CV_Phong.nguyenha_sections.txt\n",
      "Extracted content has been saved to output\\CV_Phạm Thu Trang_sections.txt\n",
      "Extracted content has been saved to output\\CV_Tran Thi Kim Bac_sections.txt\n",
      "Extracted content has been saved to output\\CV_Tran_Tien_Dat_sections.txt\n",
      "Extracted content has been saved to output\\CV_Tran_Trong_Nghia_sections.txt\n",
      "Extracted content has been saved to output\\CV_VTI_Nguyen Phuong Chi_Tester_sections.txt\n",
      "Extracted content has been saved to output\\Lương Văn Huấn_EN_sections.txt\n",
      "Extracted content has been saved to output\\Nguyễn Duyên Mạnh_EN_sections.txt\n",
      "Extracted content has been saved to output\\Phạm Thị Huê_V02316_EN_sections.txt\n",
      "Extracted content has been saved to output\\Thai Minh Tuan_sections.txt\n",
      "Extracted content has been saved to output\\Trần Nhân Tôn_V02307_EN_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV - Pham Quang Vinh_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV - Tran Minh Khuong_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Bui Duc Anh_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Chu Ngoc Minh_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Hoang Dong Hoan_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Hoang Thu Phuong_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Huynh Thanh Tu_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Nguyen Duc Thang_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Nguyen Gia Tung_sections.txt\n",
      "Extracted content has been saved to output\\VTI - CV Nguyen Phu Quan_sections.txt\n",
      "Extracted content has been saved to output\\VTI- CV Dang The Anh_sections.txt\n",
      "Extracted content has been saved to output\\VTI-CV Do Xuan Hiep_sections.txt\n",
      "Extracted content has been saved to output\\VTI-CV_Bui Manh Phuc_sections.txt\n",
      "Extracted content has been saved to output\\VTI-CV_Nguyen Cong Tuan Phuong_sections.txt\n",
      "Extracted content has been saved to output\\VTI-Luong Ba Hoang_sections.txt\n",
      "Extracted content has been saved to output\\VTI_CV_Hoang-Dang-Khoa_DevOps_sections.txt\n",
      "Extracted content has been saved to output\\VTI_CV_MaiThanhLiem_sections.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minh.khuatduynhat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\packaging\\custom.py:213: UserWarning: Unknown type for KSOProductBuildVer\n",
      "  warn(f\"Unknown type for {prop.name}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted content has been saved to output\\VTI_CV_Thai-Xuan-Phuong_DevOps_sections.txt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:42.016351Z",
     "start_time": "2025-02-25T10:06:41.950728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "section_mapping = {\n",
    "    \"PERSONAL DETAIL\": \"Personal Detail\",\n",
    "    \"PERSONAL DETAILS\": \"Personal Detail\",\n",
    "    \"ABOUT\": \"About\",\n",
    "    \"ABOUTS\": \"About\",\n",
    "    \"EDUCATIONAL BACKGROUND\": \"Educational Background\",\n",
    "    \"EDUCATION\": \"Educational Background\",\n",
    "    \"LANGUAGE\": \"Language\",\n",
    "    \"LANGUAGES\": \"Language\",\n",
    "    \"CERTIFICATION\": \"Certification\",\n",
    "    \"CERTIFICATIONS\": \"Certification\",\n",
    "    \"SKILL\": \"Technical Skill\",\n",
    "    \"SKILLS\": \"Technical Skill\",\n",
    "    \"TECHNICAL SKILL\": \"Technical Skill\",\n",
    "    \"TECHNICAL SKILLS\": \"Technical Skill\",\n",
    "    \"EXPERIENCE\": \"Experience\",\n",
    "    \"WORK EXPERIENCE\": \"Experience\",\n",
    "    \"EXPERIENCES\": \"Experience\",\n",
    "    # Add more mappings as needed\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:43.704462Z",
     "start_time": "2025-02-25T10:06:43.684817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_section_header(header):\n",
    "    \"\"\"\n",
    "    Normalize a section header using the mapping.\n",
    "    \"\"\"\n",
    "    header_clean = header.strip().upper()\n",
    "    # Return the normalized name if found; otherwise, capitalize normally.\n",
    "    return section_mapping.get(header_clean, header_clean.title())\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:45.460575Z",
     "start_time": "2025-02-25T10:06:45.456536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Split a line into key-value parts using tab or multiple spaces as delimiters.\n",
    "    \"\"\"\n",
    "    if '\\t' in line:\n",
    "        parts = line.split('\\t')\n",
    "    else:\n",
    "        parts = re.split(r'\\s{2,}', line)\n",
    "    return [part.strip() for part in parts if part.strip()]\n"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:47.263895Z",
     "start_time": "2025-02-25T10:06:47.256047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('cv.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = {}\n",
    "current_section = None\n",
    "section_content = []"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:48.627972Z",
     "start_time": "2025-02-25T10:06:48.611582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    # Check if line is a section header.\n",
    "    # Either it is all uppercase (with spaces) or it matches one of our mapped keys.\n",
    "    if re.fullmatch(r'[A-Z ]+', line) or line.strip().upper() in section_mapping:\n",
    "        # Save content of previous section, if any.\n",
    "        if current_section and section_content:\n",
    "            if current_section not in data:\n",
    "                data[current_section] = \" \".join(section_content)\n",
    "            else:\n",
    "                data[current_section] += \" \" + \" \".join(section_content)\n",
    "        # Normalize the section header.\n",
    "        current_section = normalize_section_header(line)\n",
    "        section_content = []\n",
    "        continue\n",
    "\n",
    "    # Try parsing the line as a key-value pair.\n",
    "    parts = parse_line(line)\n",
    "    if len(parts) == 2:\n",
    "        key, value = parts\n",
    "        if current_section and section_content:\n",
    "            data[current_section] = \" \".join(section_content)\n",
    "            current_section = line\n",
    "            section_content = []\n",
    "        else:\n",
    "            section_content.append(line)\n",
    "    else:\n",
    "        section_content.append(line)"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:50.645126Z",
     "start_time": "2025-02-25T10:06:50.641878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if current_section and section_content:\n",
    "    if current_section not in data:\n",
    "        data[current_section] = \" \".join(section_content)\n",
    "    else:\n",
    "        data[current_section] += \" \" + \" \".join(section_content)"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:52.432958Z",
     "start_time": "2025-02-25T10:06:52.427968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flat_data = {}\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, dict):\n",
    "        for sub_key, sub_value in value.items():\n",
    "            flat_data[f\"{key}_{sub_key}\"] = sub_value\n",
    "    elif isinstance(value, list):\n",
    "        flat_data[key] = \"; \".join(value)\n",
    "    else:\n",
    "        flat_data[key] = value\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:06:53.923944Z",
     "start_time": "2025-02-25T10:06:53.916925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('cv.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=flat_data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(flat_data)\n",
    "\n",
    "print(\"CSV file 'cv.csv' created successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'cv.csv' created successfully!\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:45:06.912285Z",
     "start_time": "2025-02-25T10:45:06.901395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Define mapping from text file headers to CSV column names\n",
    "section_headers = {\n",
    "    'PERSONAL DETAIL': 'Personal detail',\n",
    "    'ABOUT': 'About',\n",
    "    'EDUCATIONAL BACKGROUND': 'Education',\n",
    "    'LANGUAGE': 'Language',\n",
    "    'TECHNICAL SKILL': 'Technical Skill',\n",
    "    'EXPERIENCE': 'Experience'\n",
    "}\n",
    "\n",
    "# Read the text file and clean up the lines\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Prepare a dictionary to hold each section’s lines\n",
    "data = {col: [] for col in section_headers.values()}\n",
    "current_section = None\n",
    "\n",
    "# Process lines and group them by their section\n",
    "for line in lines:\n",
    "    if line in section_headers:\n",
    "        current_section = section_headers[line]\n",
    "    else:\n",
    "        if current_section:\n",
    "            data[current_section].append(line)\n",
    "\n",
    "# Function to process key–value lines: splits on 2+ spaces and joins with a colon.\n",
    "def process_key_value(lines, join_sep=';'):\n",
    "    processed = []\n",
    "    for line in lines:\n",
    "        parts = re.split(r'\\s{2,}', line)\n",
    "        if len(parts) >= 2:\n",
    "            processed.append(f\"{parts[0]}:{parts[1]}\")\n",
    "        else:\n",
    "            processed.append(parts[0])\n",
    "    return join_sep.join(processed)\n",
    "\n",
    "# Process each section as needed\n",
    "result = {}\n",
    "result['Personal detail'] = process_key_value(data['Personal detail'], join_sep=';')\n",
    "# For the ABOUT section, join all lines with a space\n",
    "result['About'] = \" \".join(data['About'])\n",
    "result['Education'] = process_key_value(data['Education'], join_sep=';')\n",
    "result['Language'] = process_key_value(data['LANGUAGE'] if 'LANGUAGE' in data else data['Language'], join_sep=';')\n",
    "result['Language'] = process_key_value(data['Language'], join_sep=';')\n",
    "result['Technical Skill'] = process_key_value(data['Technical Skill'], join_sep=';')\n",
    "result['Experience'] = process_key_value(data['Experience'], join_sep=';')\n",
    "\n",
    "# Write to CSV\n",
    "csv_columns = ['Personal detail', 'About', 'Education', 'Language', 'Technical Skill', 'Experience']\n",
    "with open('cv.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerow(result)\n",
    "\n",
    "print(\"CSV file created successfully.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:45:09.375676Z",
     "start_time": "2025-02-25T10:45:09.367998Z"
    }
   },
   "cell_type": "code",
   "source": "pd.read_csv('cv.csv')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                     Personal detail  \\\n",
       "0  Name:Le Huu Minh;Nationality:Vietnamese;DOB:19...   \n",
       "\n",
       "                                               About  \\\n",
       "0  + 6+ years of experience in Operation System +...   \n",
       "\n",
       "                                           Education  \\\n",
       "0  Now:FPT Polytechnic;Major: Business informatio...   \n",
       "\n",
       "                         Language  \\\n",
       "0  Vietnamese:Native;English:Base   \n",
       "\n",
       "                                     Technical Skill  \\\n",
       "0  Domain:Monitoring, Operation System;Monitoring...   \n",
       "\n",
       "                                          Experience  \n",
       "0  FCCL (9/2023 now);Customer:Japanese Customer;D...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personal detail</th>\n",
       "      <th>About</th>\n",
       "      <th>Education</th>\n",
       "      <th>Language</th>\n",
       "      <th>Technical Skill</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Name:Le Huu Minh;Nationality:Vietnamese;DOB:19...</td>\n",
       "      <td>+ 6+ years of experience in Operation System +...</td>\n",
       "      <td>Now:FPT Polytechnic;Major: Business informatio...</td>\n",
       "      <td>Vietnamese:Native;English:Base</td>\n",
       "      <td>Domain:Monitoring, Operation System;Monitoring...</td>\n",
       "      <td>FCCL (9/2023 now);Customer:Japanese Customer;D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:14:17.670938Z",
     "start_time": "2025-02-25T10:14:17.654615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Mapping for possible section header variations\n",
    "section_mapping = {\n",
    "    \"PERSONAL DETAIL\": \"Personal Detail\",\n",
    "    \"PERSONAL DETAILS\": \"Personal Detail\",\n",
    "    \"ABOUT\": \"About\",\n",
    "    \"ABOUTS\": \"About\",\n",
    "    \"EDUCATIONAL BACKGROUND\": \"Educational Background\",\n",
    "    \"EDUCATION\": \"Educational Background\",\n",
    "    \"LANGUAGE\": \"Language\",\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def normalize_section_header(header):\n",
    "    \"\"\"\n",
    "    Normalize a section header using the mapping.\n",
    "    \"\"\"\n",
    "    header_clean = header.strip().upper()\n",
    "    # Return the normalized name if found; otherwise, capitalize normally.\n",
    "    return section_mapping.get(header_clean, header_clean.title())\n",
    "\n",
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Split a line into key-value parts using tab or multiple spaces as delimiters.\n",
    "    \"\"\"\n",
    "    if '\\t' in line:\n",
    "        parts = line.split('\\t')\n",
    "    else:\n",
    "        parts = re.split(r'\\s{2,}', line)\n",
    "    return [part.strip() for part in parts if part.strip()]\n",
    "\n",
    "# Read the text file containing the CV data.\n",
    "with open('cv.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = {}\n",
    "current_section = None\n",
    "section_content = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    # Check if line is a section header.\n",
    "    # Either it is all uppercase (with spaces) or it matches one of our mapped keys.\n",
    "    if re.fullmatch(r'[A-Z ]+', line) or line.strip().upper() in section_mapping:\n",
    "        # Save content of previous section, if any.\n",
    "        if current_section and section_content:\n",
    "            if current_section not in data:\n",
    "                data[current_section] = \" \".join(section_content)\n",
    "            else:\n",
    "                data[current_section] += \" \" + \" \".join(section_content)\n",
    "        # Normalize the section header.\n",
    "        current_section = normalize_section_header(line)\n",
    "        section_content = []\n",
    "        continue\n",
    "\n",
    "    # Try parsing the line as a key-value pair.\n",
    "    parts = parse_line(line)\n",
    "    if len(parts) == 2:\n",
    "        key, value = parts\n",
    "        if current_section in [\"Personal Detail\", \"Language\"]:\n",
    "            if current_section == \"Language\":\n",
    "                if \"Language\" not in data:\n",
    "                    data[\"Language\"] = {}\n",
    "                data[\"Language\"][key] = value\n",
    "            else:\n",
    "                data[key] = value\n",
    "        elif current_section == \"Educational Background\":\n",
    "            # Collect multiple education entries in a list.\n",
    "            if \"Education\" not in data:\n",
    "                data[\"Education\"] = []\n",
    "            data[\"Education\"].append(f\"{key}: {value}\")\n",
    "        else:\n",
    "            # For free text sections like \"About\"\n",
    "            section_content.append(line)\n",
    "    else:\n",
    "        section_content.append(line)\n",
    "\n",
    "# Save any trailing section content.\n",
    "if current_section and section_content:\n",
    "    if current_section not in data:\n",
    "        data[current_section] = \" \".join(section_content)\n",
    "    else:\n",
    "        data[current_section] += \" \" + \" \".join(section_content)\n",
    "\n",
    "# Flatten the data for CSV writing.\n",
    "flat_data = {}\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, dict):\n",
    "        for sub_key, sub_value in value.items():\n",
    "            flat_data[f\"{key}_{sub_key}\"] = sub_value\n",
    "    elif isinstance(value, list):\n",
    "        flat_data[key] = \"; \".join(value)\n",
    "    else:\n",
    "        flat_data[key] = value\n",
    "\n",
    "# Write the data to a CSV file.\n",
    "with open('cv.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=flat_data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(flat_data)\n",
    "\n",
    "print(\"CSV file 'cv.csv' created successfully!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'cv.csv' created successfully!\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "source": [
    "# Path to the input text file and output CSV file\n",
    "input_folder = 'output'\n",
    "output_file = 'csv'\n",
    "\n",
    "# List of section names to look for (these will become CSV columns)\n",
    "section_names = [\n",
    "    'PERSONAL DETAIL',\n",
    "    'ABOUT',\n",
    "    'EDUCATIONAL BACKGROUND',\n",
    "    'CERTIFICATION',\n",
    "    'LANGUAGE',\n",
    "    'TECHNICAL SKILL',\n",
    "    'EXPERIENCE'\n",
    "]\n",
    "id=0\n",
    "# Create a CSV file to store all records\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header (section names)\n",
    "    writer.writerow(['ID'] + section_names)\n",
    "\n",
    "    # Process each text file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.txt'):  # Only process text files\n",
    "            input_file = os.path.join(input_folder, filename)\n",
    "            id=id+1\n",
    "            # Read the text file and split by section\n",
    "            with open(input_file, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Parse sections\n",
    "            data = {}\n",
    "            current_section = None\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line in section_names:\n",
    "                    current_section = line\n",
    "                    data[current_section] = []\n",
    "                elif current_section:\n",
    "                    data[current_section].append(line)\n",
    "\n",
    "            # Combine section content into a single string\n",
    "            for section in data:\n",
    "                data[section] = ' '.join(data[section]).strip()\n",
    "\n",
    "            # Create a row with all section data\n",
    "            row = [id] + [data.get(section, '') for section in section_names]\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"Structured CSV has been saved to {output_file}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UsmfpDnDeD_",
    "outputId": "ed5e7380-a138-47fa-ad71-478a904114ae",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737367478052,
     "user_tz": -420,
     "elapsed": 1410,
     "user": {
      "displayName": "snowsnow000",
      "userId": "06354588174686678047"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-02-25T10:12:52.092863Z",
     "start_time": "2025-02-25T10:12:52.073433Z"
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Create a CSV file to store all records\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m csvfile:\n\u001B[0;32m     18\u001B[0m     writer \u001B[38;5;241m=\u001B[39m csv\u001B[38;5;241m.\u001B[39mwriter(csvfile)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m# Write the header (section names)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'csv'"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "tiêu đề đang liền với văn bản"
   ],
   "metadata": {
    "id": "opIVz6aoXJUX"
   }
  }
 ]
}
