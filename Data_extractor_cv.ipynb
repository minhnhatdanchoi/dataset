{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:38:45.545300Z",
     "start_time": "2025-02-25T15:38:45.541629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:38:46.422176Z",
     "start_time": "2025-02-25T15:38:46.416905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_folder = 'EN'  # Update with your folder path\n",
    "output_folder = 'input'  # Folder to save extracted sections\n",
    "# List of section names to look for (in order)\n",
    "section_names = [\n",
    "    'PERSONAL DETAIL',\n",
    "    'ABOUT',\n",
    "    'EDUCATIONAL BACKGROUND',\n",
    "    'CERTIFICATION',\n",
    "    'LANGUAGE',\n",
    "    'TECHNICAL SKILL',\n",
    "    'EXPERIENCE'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:38:47.907795Z",
     "start_time": "2025-02-25T15:38:47.901818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_accent(text):\n",
    "    \"\"\"Removes Vietnamese accents from a string.\"\"\"\n",
    "    text = re.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', text)\n",
    "    text = re.sub(r'[ÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴ]', 'A', text)\n",
    "    text = re.sub(r'[èéẹẻẽêềếệểễ]', 'e', text)\n",
    "    text = re.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', text)\n",
    "    text = re.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', text)\n",
    "    text = re.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', text)\n",
    "    text = re.sub(r'[ìíịỉĩ]', 'i', text)\n",
    "    text = re.sub(r'[ÌÍỊỈĨ]', 'I', text)\n",
    "    text = re.sub(r'[ùúụủũưừứựửữ]', 'u', text)\n",
    "    text = re.sub(r'[ÙÚỤỦŨƯỪỨỰỬỮ]', 'U', text)\n",
    "    text = re.sub(r'[ỳýỵỷỹ]', 'y', text)\n",
    "    text = re.sub(r'[ỲÝỴỶỸ]', 'Y', text)\n",
    "    text = re.sub(r'[đ]', 'd', text)\n",
    "    text = re.sub(r'[Đ]', 'D', text)\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to find the start row of a section\n",
    "def find_section_row(section_name):\n",
    "  try:\n",
    "      return data[data.apply(lambda x: x.str.contains(section_name, case=True, na=False)).any(axis=1)].index[0]\n",
    "  except IndexError:\n",
    "      return None  # Return None if the section is not found\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "  if filename.endswith('.xlsx'):  # Only process Excel files\n",
    "    input_file = os.path.join(input_folder, filename)\n",
    "    output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_sections.txt\")\n",
    "  # Read the Excel file\n",
    "    data = pd.read_excel(input_file, header=None)  # Read without a header since this is structured data\n",
    "    data = data.astype(str)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    data = data.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Find start rows for all sections\n",
    "    sections = {name: find_section_row(name) for name in section_names}\n",
    "\n",
    "    # Filter out missing sections and sort by row index\n",
    "    sections = {name: row for name, row in sections.items() if row is not None}\n",
    "    sorted_sections = sorted(sections.items(), key=lambda x: x[1])  # Sort by start row\n",
    "\n",
    "    # Add a pseudo end row for the last section\n",
    "    end_row = find_section_row('I declare that the above information is accurate.')\n",
    "    if end_row is None:\n",
    "        end_row = data.shape[0]  # Default to the last row of the data if the end row is not found\n",
    "\n",
    "    # Process each section\n",
    "    output_content = []\n",
    "    for i, (section_name, start_row) in enumerate(sorted_sections):\n",
    "        # Determine the end row for this section\n",
    "        next_start_row = sorted_sections[i + 1][1] if i + 1 < len(sorted_sections) else end_row\n",
    "        section_data = data.iloc[start_row:next_start_row, :]\n",
    "\n",
    "        # Combine rows into a single string\n",
    "        section_text = section_data.apply(lambda row: ' '.join(row.dropna()), axis=1)\n",
    "        section_content = '\\n'.join([line.replace('nan', '').replace('- ', '').replace(' – ', '~').strip()\n",
    "                                   for line in section_text if line.strip()]).strip()\n",
    "\n",
    "        # Remove Vietnamese accents\n",
    "        section_content = remove_accent(section_content)\n",
    "\n",
    "        # Anonymize PERSONAL DETAIL section\n",
    "        if section_name == 'PERSONAL DETAIL':\n",
    "            modified_lines = []\n",
    "            for line in section_content.split('\\n'):\n",
    "                # Anonymize Name\n",
    "                if line.startswith('Name'):\n",
    "                    parts = line.split(maxsplit=1)\n",
    "                    if len(parts) == 2:\n",
    "                        name_parts = parts[1].split()\n",
    "                        if name_parts:\n",
    "                            # Keep first parts and initial of last name\n",
    "                            anonymized = ' '.join(name_parts[:-1] + [name_parts[-1][0]])\n",
    "                            line = f\"{parts[0]}    {anonymized}\"\n",
    "                # Generalize DOB\n",
    "                elif line.startswith('DOB'):\n",
    "                    parts = line.split(maxsplit=1)\n",
    "                    if len(parts) == 2:\n",
    "                        # Extract just the year\n",
    "                        year = re.search(r'\\b\\d{4}\\b', parts[1]).group() if re.search(r'\\b\\d{4}\\b', parts[1]) else ''\n",
    "                        line = f\"{parts[0]}    {year}\"\n",
    "                modified_lines.append(line)\n",
    "            section_content = '\\n'.join(modified_lines)\n",
    "\n",
    "        # Append the section content to the output\n",
    "        output_content.append(f\"{section_content}\\n\")\n",
    "\n",
    "    # Save the output to a text file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(output_content)\n",
    "\n",
    "    print(f\"Extracted content has been saved to {output_file}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0u_zS-p-8g5",
    "outputId": "c5662847-5ced-4034-aa09-488bf1e18b6c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737367473094,
     "user_tz": -420,
     "elapsed": 14114,
     "user": {
      "displayName": "snowsnow000",
      "userId": "06354588174686678047"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-02-25T15:38:58.165180Z",
     "start_time": "2025-02-25T15:38:49.650671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted content has been saved to input\\Copy of CV-Vu-Viet-Anh_sections.txt\n",
      "Extracted content has been saved to input\\Copy of VTI - CV Bui Duc Anh_sections.txt\n",
      "Extracted content has been saved to input\\CV - TO QUY THANH_sections.txt\n",
      "Extracted content has been saved to input\\CV Pham Viet Hieu - Cloud SA1_sections.txt\n",
      "Extracted content has been saved to input\\CV-Doan-Dinh-Vu-Cong_sections.txt\n",
      "Extracted content has been saved to input\\CV-Le Huu Minh_sections.txt\n",
      "Extracted content has been saved to input\\CV-Le Sy Quang_sections.txt\n",
      "Extracted content has been saved to input\\CV-Ngo-Thach-Anh_sections.txt\n",
      "Extracted content has been saved to input\\CV-Nguyen Phi Hai Nam_sections.txt\n",
      "Extracted content has been saved to input\\CV-Nguyen Thi Hong Nhung_sections.txt\n",
      "Extracted content has been saved to input\\CV-NguyenHaiMy-EN_sections.txt\n",
      "Extracted content has been saved to input\\CV-NguyenQuangHoa_sections.txt\n",
      "Extracted content has been saved to input\\CV-Pham-Thi-Minh-Luong-PM _sections.txt\n",
      "Extracted content has been saved to input\\CV-Pham-Thi-Minh_Luong_Automation Test_2024_sections.txt\n",
      "Extracted content has been saved to input\\CV-Vu-Viet-Anh_sections.txt\n",
      "Extracted content has been saved to input\\CV_Do Van Huan_sections.txt\n",
      "Extracted content has been saved to input\\CV_Luong Van Huan_sections.txt\n",
      "Extracted content has been saved to input\\CV_Nguyen Thanh Phuong_sections.txt\n",
      "Extracted content has been saved to input\\CV_Phong.nguyenha_sections.txt\n",
      "Extracted content has been saved to input\\CV_Phạm Thu Trang_sections.txt\n",
      "Extracted content has been saved to input\\CV_Tran Thi Kim Bac_sections.txt\n",
      "Extracted content has been saved to input\\CV_Tran_Tien_Dat_sections.txt\n",
      "Extracted content has been saved to input\\CV_Tran_Trong_Nghia_sections.txt\n",
      "Extracted content has been saved to input\\CV_VTI_Nguyen Phuong Chi_Tester_sections.txt\n",
      "Extracted content has been saved to input\\Lương Văn Huấn_EN_sections.txt\n",
      "Extracted content has been saved to input\\Nguyễn Duyên Mạnh_EN_sections.txt\n",
      "Extracted content has been saved to input\\Phạm Thị Huê_V02316_EN_sections.txt\n",
      "Extracted content has been saved to input\\Thai Minh Tuan_sections.txt\n",
      "Extracted content has been saved to input\\Trần Nhân Tôn_V02307_EN_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV - Pham Quang Vinh_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV - Tran Minh Khuong_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Bui Duc Anh_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Chu Ngoc Minh_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Hoang Dong Hoan_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Hoang Thu Phuong_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Huynh Thanh Tu_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Nguyen Duc Thang_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Nguyen Gia Tung_sections.txt\n",
      "Extracted content has been saved to input\\VTI - CV Nguyen Phu Quan_sections.txt\n",
      "Extracted content has been saved to input\\VTI- CV Dang The Anh_sections.txt\n",
      "Extracted content has been saved to input\\VTI-CV Do Xuan Hiep_sections.txt\n",
      "Extracted content has been saved to input\\VTI-CV_Bui Manh Phuc_sections.txt\n",
      "Extracted content has been saved to input\\VTI-CV_Nguyen Cong Tuan Phuong_sections.txt\n",
      "Extracted content has been saved to input\\VTI-Luong Ba Hoang_sections.txt\n",
      "Extracted content has been saved to input\\VTI_CV_Hoang-Dang-Khoa_DevOps_sections.txt\n",
      "Extracted content has been saved to input\\VTI_CV_MaiThanhLiem_sections.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minh1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\packaging\\custom.py:213: UserWarning: Unknown type for KSOProductBuildVer\n",
      "  warn(f\"Unknown type for {prop.name}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted content has been saved to input\\VTI_CV_Thai-Xuan-Phuong_DevOps_sections.txt\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "tiêu đề đang liền với văn bản"
   ],
   "metadata": {
    "id": "opIVz6aoXJUX"
   }
  }
 ]
}
